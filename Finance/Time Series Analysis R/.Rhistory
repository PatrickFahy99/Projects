# Now let's do a MA(3) model
set.seed(3)
x <- w <- rnorm(1000)
for (t in 4:1000) x[t] <- w[t] + 0.6*w[t-1] + 0.4*w[t-2] + 0.3*w[t-3]
layout(1:2)
plot(x, type="l")
acf(x)
x.ma <- arima(x, order=c(0, 0, 3))
x.ma
# CI for the three coefficients
0.544 + c(-1.96, 1.96)*0.0309
0.345 + c(-1.96, 1.96)*0.0349
0.298 + c(-1.96, 1.96)*0.0311
# Now let's use Financial Data
require(quantmod)
getSymbols("AMZN")
amznrt = diff(log(Cl(AMZN)))
# Fitting an MA(1) model on the differenced log returns
amznrt.ma <- arima(amznrt, order=c(0, 0, 1))
amznrt.ma
layout(1:1)
acf(amznrt.ma$res[-1])
# Plotting the autocorrelation of the MA model
acf(amznrt.ma$res[-1])
# Plotting the autocorrelation of the residuals of the MA model fitting to the differenced amazon stock returns
acf(amznrt.ma$res[-1])
# Notice that we have a few significant peaks at lags k = 2, k = 11, k = 16 and k = 18,
# indicating that the MA(1) model is unlikely to be a good fit for the behaviour of the AMZN log
# returns, since this does not look like a realisation of white noise.
# Now try MA(2):
amznrt.ma <- arima(amznrt, order=c(0, 0, 2))
amznrt.ma
acf(amznrt.ma$res[-1])
# Now MA(3)
amznrt.ma <- arima(amznrt, order=c(0, 0, 3))
amznrt.ma
acf(amznrt.ma$res[-1])
#ARMA(1,1) model with alpha_1 = 0.5, beta_1 = -0.5
set.seed(1)
x <- arima.sim(n=1000, model=list(ar=0.5, ma=-0.5))
plot(x)
#ARMA(1,1) model with alpha_1 = 0.5, beta_1 = -0.5, and plotting the autocorrelation
set.seed(1)
x <- arima.sim(n=1000, model=list(ar=0.5, ma=-0.5))
plot(x)
acf(x)
arima(x, order=c(1, 0, 1))
# Fitting an ARMA(1,1), and finding CI's for each coefficient
arima(x, order=c(1, 0, 1))
0.6785 + c(-1.96, 1.96)*0.167
-0.7311 + c(-1.96, 1.96)*0.155
#ARMA(2,2) with alpha_1 = 0.5, alpha_2 = -0.25, beta_1 = 0.5, beta_2 = -0.3
x <- arima.sim(n=1000, model=list(ar=c(0.5, -0.25), ma=c(0.5, -0.3)))
plot(x)
acf(x)
arima(x, order=c(2, 0, 2))
# CI for AR part
0.3569 + c(-1.96, 1.96)*0.1411
-0.228 + c(-1.96, 1.96)*0.0356
# CI for MA part
0.6464 + c(-1.96, 1.96)*0.1436
-0.1493 + c(-1.96, 1.96)*0.1275
# Choosing the best ARMA(p,q) model
# Simulate 1000 timestamps via an arima model
x <- arima.sim(n=1000, model=list(ar=c(0.5, -0.25, 0.4), ma=c(0.5, -0.3)))
# We want to minimise the aic, so we look at which model does so
# First we initialise the aic at infinity and iterate through potential models
# Updating the final.aic when we see one lower
final.aic <- Inf
final.order <- c(0,0,0)
for (i in 0:4) for (j in 0:4) {
current.aic <- AIC(arima(x, order=c(i, 0, j)))
if (current.aic < final.aic) {
final.aic <- current.aic
final.order <- c(i, 0, j)
final.arma <- arima(x, order=final.order)
}
}
# Return the final aic
final.aic
# Return the final order
final.order
final.arma
# Showing the coefficients
final.arma
acf(resid(final.arma))
# Plotting the autocorrelation of the residuals of the model vs our original
acf(resid(final.arma))
# Perform Ljung-Box test for 20 Lags to confirm this is good
Box.test(resid(final.arma), lag=20, type="Ljung-Box")
#0.869 >> 0.05 so really good model
# Perform Ljung-Box test for 20 Lags to confirm this is good
Box.test(resid(final.arma), lag=20, type="Ljung-Box")
#0.7133 >> 0.05 so really good model
# Financial data:
require(quantmod)
getSymbols("^GSPC")
sp = diff(log(Cl(GSPC)))
# Financial data, we do the same for the differenced log returns for the s&p500:
require(quantmod)
getSymbols("^GSPC")
sp = diff(log(Cl(GSPC)))
spfinal.aic <- Inf
spfinal.order <- c(0,0,0)
for (i in 0:4) for (j in 0:4) {
spcurrent.aic <- AIC(arima(sp, order=c(i, 0, j)))
if (spcurrent.aic < spfinal.aic) {
spfinal.aic <- spcurrent.aic
spfinal.order <- c(i, 0, j)
spfinal.arma <- arima(sp, order=spfinal.order)
}
}
spfinal.order
acf(resid(spfinal.arma), na.action=na.omit)
# Quite a few high peaks, especially at higher lags.
Box.test(resid(spfinal.arma), lag=20, type="Ljung-Box")
# p-val is very small so this model isnt very goooood
# Hence there is additional autocorrelation in the residuals
# that is not explained by the fitted ARMA(2,3) model.
set.seed(2)
# Create an ARIMA(1,1,1) model
x <- arima.sim(list(order = c(1,1,1), ar = 0.6, ma=-0.5), n = 1000)
plot(x)
x.arima <- arima(x, order=c(1, 1, 1))
x.arima <- arima(x, order=c(1, 1, 1))
x.arima <- arima(x, order=c(1, 1, 1))
arima(x, order=c(1, 1, 1))
0.6470 + c(-1.96, 1.96)*0.1065
-0.5165 + c(-1.96, 1.96)*0.1189
x.arima <- arima(x, order=c(1, 1, 1))
arima(x, order=c(1, 1, 1))
x.arima <- arima(x, order=c(1, 1, 1))
x.arima
# CI's for the coefficients
0.6470 + c(-1.96, 1.96)*0.1065
-0.5165 + c(-1.96, 1.96)*0.1189
acf(resid(x.arima))
# Ljung-Box test on the residuals
Box.test(resid(x.arima), lag=20, type="Ljung-Box")
# Financial Data:
install.packages("forecast")
library(forecast)
# Financial Data:
install.packages("forecast")
library(forecast)
require(quantmod)
getSymbols("AMZN", from="2013-01-01")
# Financial Data:
# We get stock price data for amazon and calculate the differenced log prices
# Then fit an ARIMA model to this like we did in the ARMA case
install.packages("forecast")
library(forecast)
require(quantmod)
getSymbols("AMZN", from="2013-01-01")
amzn = diff(log(Cl(AMZN)))
azfinal.aic <- Inf
azfinal.order <- c(0,0,0)
for (p in 1:4) for (d in 0:1) for (q in 1:4) {
azcurrent.aic <- AIC(arima(amzn, order=c(p, d, q)))
if (azcurrent.aic < azfinal.aic) {
azfinal.aic <- azcurrent.aic
azfinal.order <- c(p, d, q)
azfinal.arima <- arima(amzn, order=azfinal.order)
}
}
azfinal.order
amzn = diff(log(Cl(AMZN)))
azfinal.aic <- Inf
azfinal.order <- c(0,0,0)
for (p in 1:4) for (d in 0:1) for (q in 1:4) {
azcurrent.aic <- AIC(arima(amzn, order=c(p, d, q)))
if (azcurrent.aic < azfinal.aic) {
azfinal.aic <- azcurrent.aic
azfinal.order <- c(p, d, q)
azfinal.arima <- arima(amzn, order=azfinal.order)
}
}
azfinal.order
acf(resid(azfinal.arima), na.action=na.omit)
Box.test(resid(azfinal.arima), lag=20, type="Ljung-Box")
plot(forecast(azfinal.arima, h=25))
# Creating the GARCH model
set.seed(2)
a0 <- 0.2
a1 <- 0.5
b1 <- 0.3
w <- rnorm(10000)
eps <- rep(0, 10000)
sigsq <- rep(0, 10000)
for (i in 2:10000) {
sigsq[i] <- a0 + a1 * (eps[i-1]^2) + b1 * sigsq[i-1]
eps[i] <- w[i]*sqrt(sigsq[i])
}
# Plot the autocorrelation
acf(eps)
# Plot the autocorrelation for the sequence and the sequence squared
acf(eps)
acf(eps^2)
# we see substantial evidence of a conditionally heteroskedastic process via the decay of successive lags.
require(tseries)
# Finding a garch model to fit the data
eps.garch <- garch(eps, trace=FALSE)
confint(eps.garch)
# Financial data:
require(quantmod)
getSymbols("^FTSE")
ftrt = diff(log(Cl(FTSE)))
plot(ftrt)
# We also need to remove the NA value generated by the differencing procedure
ft <- as.numeric(ftrt)
ft <- ft[!is.na(ft)]x
# We also need to remove the NA value generated by the differencing procedure
ft <- as.numeric(ftrt)
ft <- ft[!is.na(ft)]
# Fit an arima model
ftfinal.aic <- Inf
ftfinal.order <- c(0,0,0)
for (p in 1:4) for (d in 0:1) for (q in 1:4) {
ftcurrent.aic <- AIC(arima(ft, order=c(p, d, q)))
if (ftcurrent.aic < ftfinal.aic) {
ftfinal.aic <- ftcurrent.aic
ftfinal.order <- c(p, d, q)
ftfinal.arima <- arima(ft, order=ftfinal.order)
}
}
ftfinal.order
acf(resid(ftfinal.arima))
# Autocorrelation plot of the residuals
acf(resid(ftfinal.arima))
acf(resid(ftfinal.arima)^2)
# Shows serial correlation
ft.garch <- garch(ft, trace=F) # trace = F says to not output each bit
ft.res <- ft.garch$res[-1]
acf(ft.res)
# Fitting a garch model
ft.garch <- garch(ft, trace=F) # trace = F says to not output each bit
ft.res <- ft.garch$res[-1]
# Autocorrelation plot of the residuals with the GARCH model
acf(ft.res)
acf(ft.res^2)
# Jeeez pretty good
Box.test(resid(ft.garch), lag=20, type="Ljung-Box")
# p-val is 0.9926!!!!!!
# Creating a random walk
set.seed(123)
z <- rep(0, 1000)
for (i in 2:1000) z[i] <- z[i-1] + rnorm(1)
plot(z, type="l")
# Plot the autocorrelation and the differenced autocorrelation
layout(1:2)
acf(z)
acf(diff(z))
# Create two new series derived from z
x <- y <- rep(0, 1000)
x <- 0.3*z + rnorm(1000)
y <- 0.6*z + rnorm(1000)
plot(x, type="l")
plot(y, type="l")
# Plot 2x-y and its autocorrelation, should be white noise
comb <- 2*x - y
plot(comb, type="l")
acf(comb)
# Looks stationary!
# Let's use the Augmented Dickey Fuller test on comb:
library("tseries")
adf.test(comb)
# p-val is very small, so when have necessary information to reject the null hypothesis that the series possesses a unit root (i.e is not stationary)
# Now Phillips-Perron test:
pp.test(comb)
# Yeet, same outcome
# Now the Phillips-Ouliaris test:
po.test(cbind(2*x,-1.0*y))
# Again what we want
# Now try the same for a bad combination of x and y:
badcomb <- -1.0*x + 2.0*y
plot(badcomb, type="l")
acf(diff(badcomb))
adf.test(badcomb)
# p-val is 0.39 so way above 0.05 so cant reject null hypothesis
comb2 = lm(x~y)
comb2 = lm(x~y)
comb2
#
comb2 = lm(x~y)
comb2
adf.test(comb2$residuals, k=1)
# Let's use the Augmented Dickey Fuller test on comb:
library("tseries")
adf.test(comb)
# p-val is very small, so when have necessary information to reject the null hypothesis that the series possesses a unit root (i.e is not stationary)
# Now Phillips-Perron test:
pp.test(comb)
# Nice, same outcome
# Now the Phillips-Ouliaris test:
po.test(cbind(2*x,-1.0*y))
# Again what we want
# Now try the same for a bad combination of x and y:
badcomb <- -1.0*x + 2.0*y
plot(badcomb, type="l")
acf(diff(badcomb))
adf.test(badcomb)
# p-val is 0.39 so way above 0.05 so cant reject null hypothesis
# Plot the ETF backward-adjusted closing prices
plot(aAdj, type="l", xlim=c(0, length(aAdj)), ylim=c(0.0, 45.0),
xlab="November 11th 2014 to January 1st 2017",
ylab="Backward-Adjusted Prices in USD", col="blue")
par(new=T)
plot(bAdj, type="l", xlim=c(0, length(bAdj)), ylim=c(0.0, 45.0),
axes=F, xlab="", ylab="", col="red")
par(new=F)
library("quantmod")
library("tseries")
layout(1:1)
# Obtain EWA and EWC time series prices
getSymbols("EWA", from="2014-11-11", to="2020-01-01")
getSymbols("EWC", from="2014-11-11", to="2020-01-01")
# Use adjusted closing prices
aAdj = unclass(EWA$EWA.Adjusted)
bAdj = unclass(EWC$EWC.Adjusted)
# Plot the ETF backward-adjusted closing prices
plot(aAdj, type="l", xlim=c(0, length(aAdj)), ylim=c(0.0, 45.0),
xlab="November 11th 2014 to January 1st 2017",
ylab="Backward-Adjusted Prices in USD", col="blue")
par(new=T)
plot(bAdj, type="l", xlim=c(0, length(bAdj)), ylim=c(0.0, 45.0),
axes=F, xlab="", ylab="", col="red")
par(new=F)
# Plot a scatter graph of the ETF adjusted prices
plot(aAdj, bAdj, xlab="ARNC Backward-Adjusted Prices",
ylab="UNG Backward-Adjusted Prices")
# Carry out linear regression on the two price series
comb = lm(aAdj~bAdj)
comb
# Now we perform the ADF test on the residuals,
# or "spread" of the model, using a single lag order
adf.test(comb$residuals, k=1)
# p-val very small so can reject null hypothesis - therefore stationary
set.seed(1)
# Create a random walk
z <- rep(0, 10000)
for (i in 2:10000) z[i] <- z[i-1] + rnorm(1)
# Create three time series derived from this random walk, with some noise
p <- q <- r <- rep(0, 10000)
p <- 0.3*z + rnorm(10000)
q <- 0.6*z + rnorm(10000)
r <- 0.2*z + rnorm(10000)
# Performing the Johansen test and p,q,r
jotest=ca.jo(data.frame(p,q,r), type="trace", K=2, ecdet="none",
spec="longrun")
# K is the number of lags >= 2
# ecdet refers to whether to use a constant or drift term in the model
# spec="longrun"
# refers to the specification of the VECM. This parameter can be spec="longrun"
# or spec="transitory".
summary(jotest)
# Performing the Johansen test and p,q,r
jotest=ca.jo(data.frame(p,q,r), type="trace", K=2, ecdet="none", spec="longrun")
# K is the number of lags >= 2
# ecdet refers to whether to use a constant or drift term in the model
# spec="longrun"
# refers to the specification of the VECM. This parameter can be spec="longrun"
# or spec="transitory".
summary(jotest)
require("urca")
set.seed(1)
# Create a random walk
z <- rep(0, 10000)
for (i in 2:10000) z[i] <- z[i-1] + rnorm(1)
# Create three time series derived from this random walk, with some noise
p <- q <- r <- rep(0, 10000)
p <- 0.3*z + rnorm(10000)
q <- 0.6*z + rnorm(10000)
r <- 0.2*z + rnorm(10000)
# Performing the Johansen test and p,q,r
jotest=ca.jo(data.frame(p,q,r), type="trace", K=2, ecdet="none", spec="longrun")
# K is the number of lags >= 2
# ecdet refers to whether to use a constant or drift term in the model
# spec="longrun"
# refers to the specification of the VECM. This parameter can be spec="longrun"
# or spec="transitory".
summary(jotest)
s = 1.000*p - 2.255 *q + 5.265*r
plot(s, type="l")
library("tseries")
adf.test(s)
# p-val is very low!!
# Financial data:
library("quantmod")
getSymbols("EWA", from="2006-04-26", to="2012-04-09")
getSymbols("EWC", from="2006-04-26", to="2012-04-09")
getSymbols("IGE", from="2006-04-26", to="2012-04-09")
ewaAdj = unclass(EWA$EWA.Adjusted)
ewcAdj = unclass(EWC$EWC.Adjusted)
igeAdj = unclass(IGE$IGE.Adjusted)
jotest=ca.jo(data.frame(ewaAdj,ewcAdj,igeAdj), type="trace",
K=2, ecdet="none", spec="longrun")
summary(jotest)
s = ewaAdj - 1.008*ewcAdj + 0.227*igeAdj
adf.test(s)
# Different instruments that all track to S&P:
getSymbols("SPY", from="2015-01-01", to="2015-12-31")
getSymbols("IVV", from="2015-01-01", to="2015-12-31")
getSymbols("VOO", from="2015-01-01", to="2015-12-31")
spyAdj = unclass(SPY$SPY.Adjusted)
ivvAdj = unclass(IVV$IVV.Adjusted)
vooAdj = unclass(VOO$VOO.Adjusted)
jotest=ca.jo(data.frame(spyAdj,ivvAdj,vooAdj), type="trace", K=2,
ecdet="none", spec="longrun")
summary(jotest)
# r \leq 2 we reject null only at the 5% level (compared to far exceeding the 1% level for 0 and 1 ) so have to be careful that r may be 2.
s = spyAdj - 0.347*ivvAdj - 0.710*vooAdj
adf.test(s)
install.packages('depmixS4')
install.packages('quantmod')
library('depmixS4')
library('quantmod')
set.seed(1)
install.packages("quantmod")
require('depmixS4')
require('quantmod')
require('depmixS4')
require('quantmod')
set.seed(1)
# Create the parameters for the bull and
# bear market returns distributions
Nk_lower <- 50
Nk_upper <- 150
bull_mean <- 0.1
bull_var <- 0.1
bear_mean <- -0.05
bear_var <- 0.2
# Create the list of durations (in days) for each regime
days <- replicate(5, sample(Nk_lower:Nk_upper, 1))
# Create the various bull and bear markets returns
market_bull_1 <- rnorm( days[1], bull_mean, bull_var )
market_bear_2 <- rnorm( days[2], bear_mean, bear_var )
market_bull_3 <- rnorm( days[3], bull_mean, bull_var )
market_bear_4 <- rnorm( days[4], bear_mean, bear_var )
market_bull_5 <- rnorm( days[5], bull_mean, bull_var )
# Create the list of true regime states and full returns list
true_regimes <- c( rep(1,days[1]), rep(2,days[2]), rep(1,days[3]),
rep(2,days[4]), rep(1,days[5]))
returns <- c( market_bull_1, market_bear_2, market_bull_3,
market_bear_4, market_bull_5)
plot(returns, type="l", ylab="Returns")
# Create and fit the Hidden Markov Model
hmm <- depmix(returns ~ 1, family = gaussian(), nstates = 2,
data=data.frame(returns=returns))
hmmfit <- fit(hmm, verbose = FALSE)
# Output both the true regimes and the
# posterior probabilities of the regimes
post_probs <- posterior(hmmfit)
layout(1:2)
plot(post_probs$state, type='s', main='True Regimes',
xlab='', ylab='Regime')
matplot(post_probs[,-1], type='l',
main='Regime Posterior Probabilities',
ylab='Probability')
legend(x='topright', c('Bull','Bear'), fill=1:2, bty='n')
# Financial data:
# Obtain S&P500 data from 2004 onwards and
# create the returns stream from this
getSymbols( "^GSPC", from="2004-01-01" )
gspcRets = diff( log( Cl( GSPC ) ) )
returns = as.numeric(gspcRets)
plot(gspcRets)
# Fit a Hidden Markov Model with two states
# to the S&P500 returns stream
hmm <- depmix(returns ~ 1, family = gaussian(), nstates = 2,
data=data.frame(returns=returns))
hmmfit <- fit(hmm, verbose = FALSE)
post_probs <- posterior(hmmfit)
# Plot the returns stream and the posterior
# probabilities of the separate regimes
layout(1:2)
plot(returns, type='l', main='Regime Detection', xlab='', ylab='Returns')
matplot(post_probs[,-1], type='l',
main='Regime Posterior Probabilities',
ylab='Probability')
legend(x='bottomleft', c('Regime #1','Regime #2'), fill=1:2, bty='n')
# Fit a Hidden Markov Model with three states
# to the S&P500 returns stream
hmm <- depmix(returns ~ 1, family = gaussian(), nstates = 3,
data=data.frame(returns=returns))
hmmfit <- fit(hmm, verbose = FALSE)
post_probs <- posterior(hmmfit)
# Plot the returns stream and the posterior
# probabilities of the separate regimes
layout(1:2)
plot(returns, type='l', main='Regime Detection',
xlab='', ylab='Returns')
matplot(post_probs[,-1], type='l',
main='Regime Posterior Probabilities',
ylab='Probability')
legend(x='bottomleft', c('Regime #1','Regime #2', 'Regime #3'),
fill=1:3, bty='n')
# Create the parameters for the bull and
# bear market returns distributions
Nk_lower <- 50
Nk_upper <- 150
bull_mean <- 0.1
bull_var <- 0.1
bear_mean <- -0.05
bear_var <- 0.2
# Create the list of durations (in days) for each regime
days <- replicate(5, sample(Nk_lower:Nk_upper, 1))
# Create the various bull and bear markets returns
market_bull_1 <- rnorm( days[1], bull_mean, bull_var )
market_bear_2 <- rnorm( days[2], bear_mean, bear_var )
market_bull_3 <- rnorm( days[3], bull_mean, bull_var )
market_bear_4 <- rnorm( days[4], bear_mean, bear_var )
market_bull_5 <- rnorm( days[5], bull_mean, bull_var )
